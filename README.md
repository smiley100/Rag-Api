# RAG API (Retrieval-Augmented Generation) ‚ú®

FRENCH

**Description rapide**

API RAG l√©g√®re d√©velopp√©e avec **FastAPI** qui combine :
- **Ollama** (LLM + embeddings)
- **ChromaDB** (stockage vectoriel)

L'application permet d'uploader des fichiers (PDF/TXT), d'indexer leur contenu et de poser des questions auxquelles le mod√®le r√©pond en s'appuyant sur les passages les plus pertinents.

---

## üîß Pr√©requis importants

- **Docker** et **Docker Compose** (v2 recommand√©e) doivent √™tre install√©s.
- **M√©moire recommand√©e : 8 GB de RAM** (fonctionne parfois avec moins, mais la qualit√©/performance peut baisser). 

> Conseil : Si tu manques de RAM, ferme d'autres gros processus et augmente la m√©moire disponible pour Docker avant de d√©marrer.

---

## ‚úÖ Premi√®re ex√©cution (tr√®s importante)

Pour la premi√®re ex√©cution, lance :

```bash
docker compose up -d --build
```

Pourquoi : le service `ollama` est configur√© pour d√©marrer via `scripts/init_ollama.sh` (il d√©marre le service Ollama et t√©l√©charge automatiquement les mod√®les requis). Lancer `docker compose up -d` la premi√®re fois garantit que les volumes sont cr√©√©s et que le script de d√©marrage s'ex√©cute correctement.

---

## ‚ö†Ô∏è Mod√®les Ollama requis

Les mod√®les suivants doivent √™tre pr√©sents dans Ollama pour que l'API fonctionne correctement :

- `nemotron-mini:latest` ‚Äî mod√®le LLM de g√©n√©ration
- `nomic-embed-text:latest` ‚Äî mod√®le d'embeddings

Le d√©marrage du service Ollama via `docker compose up -d` ex√©cutera `scripts/init_ollama.sh` (mont√© dans le conteneur) et **cela t√©l√©charge automatiquement ces mod√®les**. Si tu pr√©f√®res contr√¥ler le processus manuellement, tu peux utiliser la CLI Ollama (si install√©e localement) :

V√©rifier la pr√©sence des mod√®les (depuis la machine h√¥te ou depuis le conteneur) :

```bash
docker exec -it ollama ollama list
```

Remarque importante : **Ne modifie pas `scripts/init_ollama.sh`** ‚Äî il est fourni et configur√© pour d√©marrer Ollama et t√©l√©charger les mod√®les. Le script est d√©j√† ex√©cutable et valid√©.

---

## üöÄ Endpoints et utilisation basique

Base URL (local) : `http://localhost:5000`

- `GET /` ‚Äî v√©rification de sant√©

Exemple :

```bash
curl http://localhost:5000/
# => {"message":"‚úÖ RAG API en ligne et fonctionnelle"}
```

- `POST /upload` ‚Äî uploader un PDF ou TXT (multipart/form-data)

Exemple :

```bash
curl -X POST "http://localhost:5000/upload" -F "file=@/chemin/vers/ton_doc.pdf"
```

- `POST /ask` ‚Äî poser une question (form param `question`)

Exemple :

```bash
curl -X POST "http://localhost:5000/ask" -F "question=Quelle est la capitale du S√©n√©gal ?"
```

Le service r√©cup√®re les passages les plus similaires via ChromaDB et construit un prompt envoy√© au mod√®le pour g√©n√©rer la r√©ponse.

---

## üìÅ Volumes et persistance

- Volume Ollama : `ollama_data` (stocke les mod√®les)
- Volume ChromaDB : `chroma_data` (indexs persistants)
- Uploads temporaires : `./rag-api/data` (mont√© dans le conteneur `rag-api`)

---

## üõ†Ô∏è D√©pannage rapide

- Les mod√®les ne sont pas disponibles : v√©rifie `docker logs ollama` et/ou lance `docker exec -it ollama ollama list`.
- L'API renvoie une erreur li√©e √† Ollama : v√©rifie que `OLLAMA_URL` (dans la config/dans l'environnement du conteneur `rag-api`) pointe bien sur `http://ollama:11434`.
- Pour voir les logs :

```bash
docker-compose logs -f rag-api
docker-compose logs -f chromadb
docker-compose logs -f ollama
```

## Licence

Pas de licence sp√©cifi√©e 

ENGLISH

Bien s√ªr. Voici la **version anglaise fid√®le et propre**, pr√™te √† √™tre utilis√©e telle quelle dans ton README üëå

---

# RAG API (Retrieval-Augmented Generation) ‚ú®

**Quick description**

A lightweight RAG API built with **FastAPI** that combines:

* **Ollama** (LLM + embeddings)
* **ChromaDB** (vector storage)

The application allows you to upload documents (PDF/TXT), index their content, and ask questions answered by the model using the most relevant retrieved passages.

---

## üîß Important prerequisites

* **Docker** and **Docker Compose** (v2 recommended) must be installed.
* **Recommended memory: 8 GB of RAM** (it may run with less, but performance and quality can degrade).

> Tip: If you are low on RAM, close heavy applications and increase the memory available to Docker before starting.

---

## ‚úÖ First run (very important)

For the first execution, run:

```bash
docker compose up -d --build
```

Why: Running `docker exec -it ollama /bin/sh /scripts/init_ollama.sh`  ensures required models are correctly been downloaded in ollama container.

---

## ‚ö†Ô∏è Required Ollama models

The following models must be available in Ollama for the API to work properly:

* `nemotron-mini:latest` ‚Äî LLM for text generation
* `nomic-embed-text:latest` ‚Äî embedding model


If you prefer to check manually, you can use the Ollama CLI:

```bash
docker exec -it ollama ollama list
```

Important note: **Do not modify `scripts/init_ollama.sh`**. It is provided and configured to start Ollama and pull the required models.

---

## üöÄ Endpoints and basic usage

Base URL (local): `http://localhost:5000`

* `GET /` ‚Äî health check

Example:

```bash
curl http://localhost:5000/
# => {"message":"‚úÖ RAG API is up and running"}
```

* `POST /upload` ‚Äî upload a PDF or TXT file (multipart/form-data)

Example:

```bash
curl -X POST "http://localhost:5000/upload" -F "file=@/path/to/your_doc.pdf"
```

* `POST /ask` ‚Äî ask a question (form parameter `question`)

Example:

```bash
curl -X POST "http://localhost:5000/ask" -F "question=What is the capital of Senegal?"
```

The service retrieves the most relevant chunks from ChromaDB and builds a prompt sent to the LLM to generate the answer.

---

## üìÅ Volumes and persistence

* Ollama volume: `ollama_data` (stores models)
* ChromaDB volume: `chroma_data` (persistent vector indexes)
* Temporary uploads: `./rag-api/data` (mounted inside the `rag-api` container)

---

## üõ†Ô∏è Quick troubleshooting

* Models not available: check `docker logs ollama` or run `docker exec -it ollama ollama list`.
* API errors related to Ollama: ensure `OLLAMA_URL` (in the `rag-api` container environment) points to `http://ollama:11434`.
* View logs:

```bash
docker compose logs -f rag-api
docker compose logs -f chromadb
docker compose logs -f ollama
```

---

## License

No license specified 
